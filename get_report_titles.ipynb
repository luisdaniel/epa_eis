{
 "metadata": {
  "name": "",
  "signature": "sha256:421858461ea72a706c3871f82c4115cea19dcc0bc195111782d6d1285973428c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup as bs\n",
      "import re\n",
      "import urllib2\n",
      "import csv\n",
      "import json\n",
      "import datetime\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns = ['eis_number', 'file_link', 'file_title']\n",
      "df = pd.DataFrame(columns=columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#test\n",
      "url = 'http://yosemite.epa.gov/oeca/webeis.nsf/EIS01/9F1351654AE8BA1485257C82002113CF'\n",
      "#url_2 = 'http://yosemite.epa.gov/oeca/webeis.nsf/EIS01/C3B3CE406F34789F85257D8700216C98'\n",
      "page = urllib2.urlopen(url).read()\n",
      "soup = bs(page)\n",
      "table = soup.findAll('td')\n",
      "check_row = table[30]\n",
      "comment_letter_links = []\n",
      "document_links = []\n",
      "exclude_links = ['http://www.epa.gov/epahome/pdf.html',\n",
      "                 'http://www.epa.gov/compliance/contact/nepa.html#commentform']\n",
      "if check_row.find(text=re.compile(r'\\bComment\\sLetter\\(s\\)')):\n",
      "    base_file_url = 'http://yosemite.epa.gov/oeca/webeis.nsf/'\n",
      "    links = table[30].findAll('a')\n",
      "    for link in links:\n",
      "        if link['href'] not in exclude_links:\n",
      "            comment_letter_links.append(base_file_url + urllib2.quote(link['href'].encode('utf8').replace('?OpenElement', '').replace('../','')))\n",
      "    links = table[31].findAll('a')\n",
      "    for link in links:\n",
      "        document_links.append(base_file_url + urllib2.quote(link['href'].encode('utf8').replace('?OpenElement', '').replace('../','')))\n",
      "else: \n",
      "    links = table[30].findAll('a')\n",
      "    for link in links:\n",
      "        document_links.append(base_file_url + urllib2.quote(link['href'].encode('utf8').replace('?OpenElement', '').replace('../','')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#1 - date\n",
      "#2 - agency\n",
      "#3 - state\n",
      "#4 - document_type\n",
      "#5 - title\n",
      "#6 - report_link\n",
      "#\"\"\"'eis_number', 'federal_register_date', 'contact_name', 'comment_due_review_date', 'contact_phone', \n",
      "#           'amended_notice_date', 'amended_notice', 'supplemental_info', 'website', 'comment_letter_date', 'comment_letter',\n",
      "#           'rating', 'num_files', 'list_of_links'\"\"\"\n",
      "\n",
      "\n",
      "#EPA server won't let you get all the URL's. Had to split it in two. \n",
      "index = 0\n",
      "with open('eis_links.csv', mode='r') as infile:\n",
      "    reader = csv.reader(infile)\n",
      "    for row in reader:\n",
      "        if row[6] != 'report_link': #only used when split in parts\n",
      "            print \"Getting # \" + str(index) + \": \" + row[6]\n",
      "            try:\n",
      "                page = urllib2.urlopen(row[6]).read()\n",
      "            except Exception, e:\n",
      "                print \"Error, could not get page: \" + str(e)\n",
      "            soup = bs(page)\n",
      "            table = soup.findAll('td')\n",
      "            eis_number = table[3].text\n",
      "            #sometimes you have comment letters which changes the order of the table rows\n",
      "            check_row = table[30]\n",
      "            documents = []\n",
      "            if check_row.find(text=re.compile(r'\\bComment\\sLetter\\(s\\)')):\n",
      "                base_file_url = 'http://yosemite.epa.gov/oeca/webeis.nsf/'\n",
      "                links = table[31].findAll('a')\n",
      "            else:\n",
      "                links = table[30].findAll('a')\n",
      "            for link in links:\n",
      "                link_formatted = base_file_url + urllib2.quote(link['href'].encode('utf8').replace('?OpenElement', '').replace('../',''))\n",
      "                documents.append({\n",
      "                    \"link\":link_formatted,\n",
      "                    \"title\":link.getText()})\n",
      "            for doc in documents:\n",
      "                new_row = [eis_number, doc['link'], doc['title']]\n",
      "                for i in range(len(new_row)):  # For every value in our newrow\n",
      "                    if hasattr(new_row[i], 'encode'):\n",
      "                        new_row[i] = new_row[i].encode('utf8')\n",
      "                df.loc[index] = new_row\n",
      "                index +=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#title of reports\n",
      "df.to_csv('report_titles.csv', encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "(4816, 3)"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}