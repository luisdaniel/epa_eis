{
 "metadata": {
  "name": "",
  "signature": "sha256:307f8b64ab339693fdb82724f6d31a954c4465f198c57b4aee1bfce9a6f4efe6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import stuff\n",
      "import json\n",
      "from boto.s3.connection import S3Connection\n",
      "from boto.s3.key import Key\n",
      "import codecs\n",
      "import csv\n",
      "import os\n",
      "import sys\n",
      "import urllib2\n",
      "from datetime import datetime\n",
      "from bs4 import BeautifulSoup as bs\n",
      "import re\n",
      "import json\n",
      "from mongoengine import *\n",
      "import models\n",
      "import bson\n",
      "from bson import json_util\n",
      "import pandas as pd\n",
      "import httplib\n",
      "import subprocess\n",
      "\n",
      "#PDF Miner\n",
      "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
      "from pdfminer.converter import TextConverter\n",
      "from pdfminer.layout import LAParams\n",
      "from pdfminer.pdfpage import PDFPage\n",
      "from cStringIO import StringIO\n",
      "\n",
      "#get keys\n",
      "json_data=open('k.json')\n",
      "keys = json.load(json_data)\n",
      "json_data.close()\n",
      "HOST = keys['es_host']\n",
      "USER = keys['es_user']\n",
      "PASS = keys['es_pass']\n",
      "MONGO = keys['mongo_uri']\n",
      "INDEX = 'impactstatement'\n",
      "PARENT = 'report'\n",
      "\n",
      "#connect to ES\n",
      "from elasticsearch import Elasticsearch as ES, RequestsHttpConnection as RC \n",
      "host_params = {'host':keys['es_host'], 'port':80, 'use_ssl':False}\n",
      "es = ES([host_params], connection_class=RC, http_auth=(keys['es_user'], keys['es_pass']),  use_ssl=False)\n",
      "\n",
      "#connect to S3\n",
      "conn = S3Connection(keys['aws_key'], keys['aws_secret'])\n",
      "bucket = conn.get_bucket('epaeis')\n",
      "bucket.list()\n",
      "\n",
      "#connect to mongo\n",
      "connect('db', host=MONGO)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "MongoClient(u'ds051640.mongolab.com', 51640)"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "base_url = \"http://yosemite.epa.gov/oeca/webeis.nsf/viEIS01!OpenView&Start=\"\n",
      "document_base_url=\"http://yosemite.epa.gov/oeca/webeis.nsf/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_max_records():\n",
      "    page = urllib2.urlopen(base_url + str(1)).read()\n",
      "    soup = bs(page)\n",
      "    records = soup.findAll(text=re.compile(r'\\bdocuments\\swere\\s\\sretrieved'))[0]\n",
      "    max_records = [int(s) for s in records.split() if s.isdigit()][0]\n",
      "    return max_records"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_eis_links_from_epa():\n",
      "    data = {\n",
      "        'date': [],\n",
      "        'agency': [],\n",
      "        'state': [],\n",
      "        'document_type': [],\n",
      "        'title': [],\n",
      "        'report_link':[]\n",
      "    }\n",
      "    increment = 29\n",
      "    max_records = get_max_records()\n",
      "    for page_num in range(1, max_records, increment):\n",
      "        page = urllib2.urlopen(base_url + str(page_num)).read()\n",
      "        print \"Getting page: \" + base_url + str(page_num)\n",
      "        soup = bs(page)\n",
      "        table = soup.findAll('tr', attrs={\"class\":\"viewdata\"})\n",
      "        for row in table:\n",
      "            data['date'].append(row.findAll('td')[0].text)\n",
      "            data['agency'].append(row.findAll('td')[1].text)\n",
      "            data['state'].append(row.findAll('td')[2].text)\n",
      "            data['document_type'].append(row.findAll('td')[3].text)\n",
      "            data['title'].append(row.findAll('td')[4].text)\n",
      "            data['report_link'].append(document_base_url + row.findAll('td')[4].find('a')['href'].replace('?opendocument', ''))\n",
      "    dataFrame = pd.DataFrame(data)\n",
      "    df_unique = dataFrame.drop_duplicates()\n",
      "    df_unique.to_csv('eis_links.csv', encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_eis_links_from_epa()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cross_check_with_mongo():\n",
      "    missing_data = {'date':[], 'title':[], 'report_link':[]}\n",
      "    with open('eis_links.csv', mode='r') as infile:\n",
      "        reader = csv.reader(infile)\n",
      "        for row in reader:\n",
      "            if row[6] != 'title':\n",
      "                report = models.Report.objects(title=row[6].strip()).first()\n",
      "                if not report:\n",
      "                    missing_data['date'].append(row[2])\n",
      "                    missing_data['title'].append(row[6].strip())\n",
      "                    missing_data['report_link'].append(row[4])\n",
      "    dataFrame = pd.DataFrame(missing_data)\n",
      "    dataFrame.to_csv('missing_reports_from_mongo.csv', encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cross_check_with_mongo()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_missing_data_for_mongo():\n",
      "    data = {\n",
      "        'agency':[],\n",
      "        'amended_notice':[],\n",
      "        'amended_notice_date':[],\n",
      "        'comment_due_review_date':[],\n",
      "        'comment_letter_date':[],\n",
      "        'comment_letters': [],\n",
      "        'comment_letters_titles': [],\n",
      "        'contact_name':[],\n",
      "        'contact_phone':[],\n",
      "        'date_uploaded':[],\n",
      "        'document_type':[],\n",
      "        'eis_number':[],\n",
      "        'federal_register_date':[],\n",
      "        'rating':[],\n",
      "        'report_files': [],\n",
      "        'report_files_titles': [],\n",
      "        'report_link':[],\n",
      "        'state':[],\n",
      "        'supplemental_info': [],\n",
      "        'title': [],\n",
      "        'website': []\n",
      "    }\n",
      "    #date = row[1]\n",
      "    #report_link = row[2]\n",
      "    #title = row[3]\n",
      "    with open('missing_reports_from_mongo.csv', mode='r') as infile:\n",
      "        reader = csv.reader(infile)\n",
      "        for row in reader:\n",
      "            if row[3] != 'title':\n",
      "                print \"Getting: \" + row[2]\n",
      "                page = urllib2.urlopen(row[2]).read()\n",
      "                soup = bs(page)\n",
      "                table = soup.findAll('td')\n",
      "                data['agency'].append(table[9].text)\n",
      "                data['amended_notice'].append(table[21].text)\n",
      "                data['amended_notice_date'].append(table[19].text)\n",
      "                data['comment_due_review_date'].append(table[15].text)\n",
      "                data['comment_letter_date'].append(table[27].text.rstrip())\n",
      "                data['contact_name'].append(table[13].text)\n",
      "                data['contact_phone'].append(table[17].text)\n",
      "                data['date_uploaded'].append(table[11].text)\n",
      "                data['document_type'].append(table[7].text)\n",
      "                data['eis_number'].append(table[3].text)\n",
      "                data['federal_register_date'].append(table[11].text)\n",
      "                data['rating'].append(table[29].text)\n",
      "                data['report_link'].append(row[2])\n",
      "                data['state'].append('00' if table[5].text == 'Multi' else table[5].text)\n",
      "                data['supplemental_info'].append(table[23].text)\n",
      "                data['title'].append(row[3])\n",
      "                data['website'].append(table[25].text.rstrip())\n",
      "                #sometimes you have comment letters which changes the order of the table rows\n",
      "                exclude_links = [\n",
      "                        'http://www.epa.gov/epahome/pdf.html',\n",
      "                        'http://www.epa.gov/compliance/contact/nepa.html#commentform']\n",
      "                if table[30].find(text=re.compile(r'\\bComment\\sLetter\\(s\\)')):\n",
      "                    data['comment_letters'].append('||'.join([l['href'] for l in table[30].findAll('a') if l['href'] not in exclude_links]))\n",
      "                    data['comment_letters_titles'].append('||'.join([l.getText() for l in table[30].findAll('a') if l['href'] not in exclude_links]))\n",
      "                    data['report_files'].append('||'.join([l['href'] for l in table[31].findAll('a')]))\n",
      "                    data['report_files_titles'].append('||'.join([l.getText() for l in table[31].findAll('a')]))\n",
      "                else:\n",
      "                    data['comment_letters'].append(\"\")\n",
      "                    data['comment_letters_titles'].append(\"\")\n",
      "                    data['report_files'].append('||'.join([l['href'] for l in table[30].findAll('a')]))\n",
      "                    data['report_files_titles'].append('||'.join([l.getText() for l in table[30].findAll('a')]))\n",
      "    dataFrame = pd.DataFrame(data)\n",
      "    dataFrame.to_csv('reports_to_be_added_to_mongo.csv', encoding='utf-8')\n",
      "    if dataFrame.shape[0]:\n",
      "        return True\n",
      "    else:\n",
      "        return False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_missing_data_for_mongo()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_full_csv_of_reports():\n",
      "    data = {\n",
      "        'agency':[],\n",
      "        'amended_notice':[],\n",
      "        'amended_notice_date':[],\n",
      "        'comment_due_review_date':[],\n",
      "        'comment_letter_date':[],\n",
      "        'comment_letters': [],\n",
      "        'comment_letters_titles': [],\n",
      "        'contact_name':[],\n",
      "        'contact_phone':[],\n",
      "        'date_uploaded':[],\n",
      "        'document_type':[],\n",
      "        'eis_number':[],\n",
      "        'federal_register_date':[],\n",
      "        'rating':[],\n",
      "        'report_files': [],\n",
      "        'report_files_titles': [],\n",
      "        'report_link':[],\n",
      "        'state':[],\n",
      "        'supplemental_info': [],\n",
      "        'title': [],\n",
      "        'website': []\n",
      "    }\n",
      "    #date = row[2]\n",
      "    #report_link = row[4]\n",
      "    #title = row[6]\n",
      "    with open('eis_links.csv', mode='r') as infile:\n",
      "        reader = csv.reader(infile)\n",
      "        for row in reader:\n",
      "            if row[4] != 'report_link':\n",
      "                print \"Getting: \" + row[4]\n",
      "                page = urllib2.urlopen(row[4]).read()\n",
      "                soup = bs(page)\n",
      "                table = soup.findAll('td')\n",
      "                data['agency'].append(table[9].text)\n",
      "                data['amended_notice'].append(table[21].text)\n",
      "                data['amended_notice_date'].append(table[19].text)\n",
      "                data['comment_due_review_date'].append(table[15].text)\n",
      "                data['comment_letter_date'].append(table[27].text.rstrip())\n",
      "                data['contact_name'].append(table[13].text)\n",
      "                data['contact_phone'].append(table[17].text)\n",
      "                data['date_uploaded'].append(table[11].text)\n",
      "                data['document_type'].append(table[7].text)\n",
      "                data['eis_number'].append(table[3].text)\n",
      "                data['federal_register_date'].append(table[11].text)\n",
      "                data['rating'].append(table[29].text)\n",
      "                data['report_link'].append(row[2])\n",
      "                data['state'].append('00' if table[5].text == 'Multi' else table[5].text)\n",
      "                data['supplemental_info'].append(table[23].text)\n",
      "                data['title'].append(row[6])\n",
      "                data['website'].append(table[25].text.rstrip())\n",
      "                #sometimes you have comment letters which changes the order of the table rows\n",
      "                exclude_links = [\n",
      "                        'http://www.epa.gov/epahome/pdf.html',\n",
      "                        'http://www.epa.gov/compliance/contact/nepa.html#commentform']\n",
      "                if table[30].find(text=re.compile(r'\\bComment\\sLetter\\(s\\)')):\n",
      "                    data['comment_letters'].append('||'.join([l['href'] for l in table[30].findAll('a') if l['href'] not in exclude_links]))\n",
      "                    data['comment_letters_titles'].append('||'.join([l.getText() for l in table[30].findAll('a') if l['href'] not in exclude_links]))\n",
      "                    data['report_files'].append('||'.join([l['href'] for l in table[31].findAll('a')]))\n",
      "                    data['report_files_titles'].append('||'.join([l.getText() for l in table[31].findAll('a')]))\n",
      "                else:\n",
      "                    data['comment_letters'].append(\"\")\n",
      "                    data['comment_letters_titles'].append(\"\")\n",
      "                    data['report_files'].append('||'.join([l['href'] for l in table[30].findAll('a')]))\n",
      "                    data['report_files_titles'].append('||'.join([l.getText() for l in table[30].findAll('a')]))\n",
      "    dataFrame = pd.DataFrame(data)\n",
      "    dataFrame.to_csv('reports.csv', encoding='utf-8')\n",
      "    if dataFrame.shape[0]:\n",
      "        return True\n",
      "    else:\n",
      "        return False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "create_full_csv_of_reports()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_reports_to_mongo():\n",
      "    data = pd.read_csv('reports_to_be_added_to_mongo.csv')\n",
      "    data = data.fillna('')\n",
      "    for row in data.T.iteritems():\n",
      "        if not models.Report.objects(eis_number=row[1]['eis_number']):\n",
      "            r = models.Report()\n",
      "            r['agency'] = [row[1]['agency']]\n",
      "            r['amended_notice'] = row[1]['amended_notice'].strip()\n",
      "            r['amended_notice_date'] = row[1]['amended_notice_date'].strip()\n",
      "            r['comment_due_review_date'] = row[1]['comment_due_review_date'].strip()\n",
      "            r['comment_letter_date'] = row[1]['comment_letter_date'].strip()\n",
      "            r['contact_name'] = row[1]['contact_name'].strip()\n",
      "            r['contact_phone'] = row[1]['contact_phone'].strip()\n",
      "            r['date_uploaded'] = row[1]['date_uploaded'].strip()\n",
      "            r['document_type'] = row[1]['document_type'].strip()\n",
      "            r['eis_number'] = str(row[1]['eis_number'])\n",
      "            r['federal_register_date'] = row[1]['federal_register_date'].strip()\n",
      "            r['rating'] = row[1]['rating'].strip()\n",
      "            r['report_link'] = row[1]['report_link'].strip()\n",
      "            r['state'] = [row[1]['state']]\n",
      "            r['supplemental_info'] = row[1]['supplemental_info'].strip()\n",
      "            r['title'] = row[1]['title'].strip()\n",
      "            r['website'] = row[1]['website'].strip()\n",
      "            r.save()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "add_reports_to_mongo()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_metadata(file_url):\n",
      "    c = httplib.HTTPConnection('yosemite.epa.gov:80')\n",
      "    c.request('HEAD', file_url)\n",
      "    try:\n",
      "        r = c.getresponse()\n",
      "    except Exception, e:\n",
      "        print \"Error, could not get link: \" + str(e)\n",
      "    if r.status == 200:\n",
      "        header_response = r.getheaders()\n",
      "        header = {}\n",
      "        for h in header_response:\n",
      "            header[h[0]] = h[1]\n",
      "    else:\n",
      "        print \"Error, could not get link: \" + str(r.status)\n",
      "    return header"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def format_url_for_s3(file_url, eis, date):\n",
      "    base_url = 'https://s3.amazonaws.com/epaeis/'\n",
      "    file_name = urllib2.unquote(file_url.strip().replace(\"?OpenElement\",\"\")).replace('../',document_base_url)\n",
      "    file_name = file_name[file_name.index('$file')+6:].replace(\".pdf\",\"\")\n",
      "    file_name = re.sub(r'([^\\s\\w])+', '', file_name).replace(\" \", \"-\").lower() + \".pdf\"\n",
      "    file_name = datetime.strptime(date, \"%m/%d/%Y\").strftime('%m-%d-%Y') + \"/\" + str(eis) + \"/\" + file_name\n",
      "    file_name = base_url + file_name\n",
      "    return file_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_info_for_missing_report_files():\n",
      "    files = {\n",
      "        'eis_number':[],\n",
      "        'content_length':[],\n",
      "        'content_type':[],\n",
      "        'date_retrieved':[],\n",
      "        'title':[],\n",
      "        'file_url_epa':[],\n",
      "        'file_url_s3':[],\n",
      "        'last_modified':[]\n",
      "    }\n",
      "    data = pd.read_csv('reports_to_be_added_to_mongo.csv')\n",
      "    data = data.fillna('')\n",
      "    c = httplib.HTTPConnection('yosemite.epa.gov:80')\n",
      "    for row in data.T.iteritems():\n",
      "        eis = row[1]['eis_number']\n",
      "        new_links = row[1]['report_files'].split('||')\n",
      "        new_titles = row[1]['report_files_titles'].split('||')\n",
      "        report = models.Report.objects(eis_number=str(eis)).first()\n",
      "        if report:\n",
      "            existing_links = [f.file_url_epa for f in report.report_files]\n",
      "            for i, l in enumerate(new_links):\n",
      "                link = urllib2.quote(l.strip().replace(\"?OpenElement\",\"\")).replace('../',document_base_url)\n",
      "                if link and link not in existing_links:\n",
      "                    try:\n",
      "                        epa_link = format_url_for_s3(link, eis, row[1]['federal_register_date'])\n",
      "                        header = get_metadata(link)\n",
      "                        files['eis_number'].append(str(eis))\n",
      "                        files['content_length'].append(header['content-length'])\n",
      "                        files['content_type'].append(header['content-type'])\n",
      "                        files['date_retrieved'].append(datetime.now())\n",
      "                        files['title'].append(new_titles[i])\n",
      "                        files['file_url_epa'].append(link)\n",
      "                        files['file_url_s3'].append(epa_link)\n",
      "                        files['last_modified'].append(header['last-modified'])\n",
      "                    except Exception, e:\n",
      "                        print \"Could not get: \" + link\n",
      "                        print \"Because: \" + str(e)\n",
      "        else: \n",
      "            print \"Cannot find: \" + str(eis)\n",
      "    dataFrame = pd.DataFrame(files)\n",
      "    dataFrame.to_csv('file_metadata_to_mongo.csv', encoding='utf-8')\n",
      "    if dataFrame.shape[0]:\n",
      "        return True\n",
      "    else:\n",
      "        return False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "files = get_info_for_missing_report_files()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update_files_to_mongo():\n",
      "    data = pd.read_csv('file_metadata_to_mongo.csv')\n",
      "    data = data.fillna('')\n",
      "    for row in data.T.iteritems():\n",
      "        eis = row[1]['eis_number']\n",
      "        report = models.Report.objects(eis_number=str(eis)).first()\n",
      "        if report:\n",
      "            print \"Adding: \" + row[1]['file_url_s3']\n",
      "            rf = models.ReportFile()\n",
      "            rf.content_length = row[1]['content_length']\n",
      "            rf.content_type = row[1]['content_type']\n",
      "            rf.date_retrieved = row[1]['date_retrieved']\n",
      "            rf.file_url_epa = row[1]['file_url_epa']\n",
      "            rf.file_url_s3 = row[1]['file_url_s3']\n",
      "            rf.last_modified = row[1]['last_modified']\n",
      "            rf.title = row[1]['title']\n",
      "            rf.converted_to_text = False\n",
      "            report.report_files.append(rf)\n",
      "            report.save()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "update_files_to_mongo()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_info_for_missing_comment_letters():\n",
      "    files = {\n",
      "        'eis_number':[],\n",
      "        'content_length':[],\n",
      "        'content_type':[],\n",
      "        'date_retrieved':[],\n",
      "        'title':[],\n",
      "        'file_url_epa':[],\n",
      "        'file_url_s3':[],\n",
      "        'last_modified':[]\n",
      "    }\n",
      "    data = pd.read_csv('reports_to_be_added_to_mongo.csv')\n",
      "    data = data.fillna('')\n",
      "    c = httplib.HTTPConnection('yosemite.epa.gov:80')\n",
      "    for row in data.T.iteritems():\n",
      "        eis = row[1]['eis_number']\n",
      "        new_links = row[1]['comment_letters'].split('||')\n",
      "        new_titles = row[1]['comment_letters_titles'].split('||')\n",
      "        report = models.Report.objects(eis_number=str(eis)).first()\n",
      "        if report:\n",
      "            existing_links = [f.file_url_epa for f in report.comment_letters]\n",
      "            for i, l in enumerate(new_links):\n",
      "                link = urllib2.quote(l.strip().replace(\"?OpenElement\",\"\")).replace('../',document_base_url)\n",
      "                if link and link not in existing_links:\n",
      "                    try:\n",
      "                        epa_link = format_url_for_s3(link, eis, row[1]['federal_register_date'])\n",
      "                        header = get_metadata(link)\n",
      "                        files['eis_number'].append(str(eis))\n",
      "                        files['content_length'].append(header['content-length'])\n",
      "                        files['content_type'].append(header['content-type'])\n",
      "                        files['date_retrieved'].append(datetime.now())\n",
      "                        files['title'].append(new_titles[i])\n",
      "                        files['file_url_epa'].append(link)\n",
      "                        files['file_url_s3'].append(epa_link)\n",
      "                        files['last_modified'].append(header['last-modified'])\n",
      "                    except Exception, e:\n",
      "                        print \"Could not get: \" + link\n",
      "                        print \"Because: \" + str(e)\n",
      "        else: \n",
      "            print \"Cannot find: \" + str(eis)\n",
      "    dataFrame = pd.DataFrame(files)\n",
      "    dataFrame.to_csv('comment_letter_metadata_to_mongo.csv', encoding='utf-8')\n",
      "    if dataFrame.shape[0]:\n",
      "        return True\n",
      "    else:\n",
      "        return False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_info_for_missing_comment_letters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update_comment_letters_to_mongo():\n",
      "    data = pd.read_csv('comment_letter_metadata_to_mongo.csv')\n",
      "    data = data.fillna('')\n",
      "    for row in data.T.iteritems():\n",
      "        eis = row[1]['eis_number']\n",
      "        report = models.Report.objects(eis_number=str(eis)).first()\n",
      "        if report:\n",
      "            print \"Adding: \" + row[1]['file_url_s3']\n",
      "            rf = models.CommentLetter()\n",
      "            rf.content_length = row[1]['content_length']\n",
      "            rf.content_type = row[1]['content_type']\n",
      "            rf.date_retrieved = row[1]['date_retrieved']\n",
      "            rf.file_url_epa = row[1]['file_url_epa']\n",
      "            rf.file_url_s3 = row[1]['file_url_s3']\n",
      "            rf.last_modified = row[1]['last_modified']\n",
      "            rf.title = row[1]['title']\n",
      "            rf.converted_to_text = False\n",
      "            report.comment_letters.append(rf)\n",
      "            report.save()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "update_comment_letters_to_mongo()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def upload_files_to_s3():\n",
      "    k = Key(bucket)\n",
      "    data = pd.read_csv('reports_to_be_added_to_mongo.csv')\n",
      "    data = data.fillna('')\n",
      "    files_to_add = list(set([str(row[1]['eis_number']) for row in data.T.iteritems()]))\n",
      "    for eis in files_to_add:\n",
      "        report = models.Report.objects(eis_number=eis).first()\n",
      "        if report:\n",
      "            links_s3 = [f.file_url_s3.replace(\"https://s3.amazonaws.com/epaeis/\",\"\") for f in report.report_files]\n",
      "            links_epa = [f.file_url_epa for f in report.report_files]\n",
      "            links_s3 = links_s3 + [f.file_url_s3.replace(\"https://s3.amazonaws.com/epaeis/\",\"\") for f in report.comment_letters]\n",
      "            links_epa = links_epa + [f.file_url_epa for f in report.comment_letters]\n",
      "            for i, l in enumerate(links_s3):\n",
      "                if bucket.get_key(links_s3[i]):\n",
      "                   print \"File exists, skipping\"\n",
      "                else:\n",
      "                    try:\n",
      "                        print \"Saving: \" + links_s3[i]\n",
      "                        f = urllib2.urlopen(links_epa[i])\n",
      "                        data = f.read()\n",
      "                        f.close()\n",
      "                        k = bucket.new_key(links_s3[i])\n",
      "                        k.set_contents_from_string(data)\n",
      "                    except Exception, e:\n",
      "                        print \"Could not get because: \" + str(e)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "upload_files_to_s3()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#----------------------------------CONVERT PDFS TO TEXT------------------------------"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def convert_files_to_text():\n",
      "    data = {\n",
      "        \"eis_number\":[],\n",
      "        \"link\":[],\n",
      "        \"error\":[]\n",
      "    }\n",
      "    reports = models.Report.objects().only('report_files.file_url_s3', 'report_files.converted_to_text')\n",
      "    for r in reports:\n",
      "        for rf in r.report_files:\n",
      "            if not rf.converted_to_text:\n",
      "                print \"Processing: \" + rf.file_url_s3\n",
      "                key = bucket.get_key(rf.file_url_s3.replace(\"https://s3.amazonaws.com/epaeis/\",\"\"))\n",
      "                key.get_contents_to_filename('temp_file.pdf')\n",
      "                try:\n",
      "                    subprocess.check_output(['pdftotext', \"temp_file.pdf\"])\n",
      "                    try:\n",
      "                        new_key = Key(bucket)\n",
      "                        new_key.key = key.name.replace(\".pdf\", \".txt\")\n",
      "                        f = open(\"temp_file.txt\", 'r')\n",
      "                        new_key.set_contents_from_file(f)\n",
      "                        f.close()\n",
      "                        rf.converted_to_text = True\n",
      "                        r.save()\n",
      "                    except Exception, e:\n",
      "                        print \"Could not save to S3: \" + str(e)\n",
      "                        data['eis_number'].append(r.eis_number)\n",
      "                        data['link'].append(rf.file_url_s3)\n",
      "                        data['error'].append(str(e))\n",
      "                except Exception, e:\n",
      "                    print \"Could not convert PDF: \" + str(e)\n",
      "                    data['eis_number'].append(r.eis_number)\n",
      "                    data['link'].append(rf.file_url_s3)\n",
      "                    data['error'].append(str(e))\n",
      "    dataFrame = pd.DataFrame(data)\n",
      "    dataFrame.to_csv('unable_to_convert_to_text.csv', encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "convert_files_to_text()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#---------------------------INDEX NEW REPORTS-----------------------------------"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Check to see which reports have not been indexed\n",
      "#loop through all of mongo the first time to sync\n",
      "#use reports_to_be_added_to_mongo.csv the second time\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def index_missing_reports():\n",
      "    data = pd.read_csv('reports_to_be_added_to_mongo.csv')\n",
      "    data = data.fillna('')\n",
      "    missing_reports = [row[1]['eis_number'] for row in data.T.iteritems()]\n",
      "    for eis in missing_reports:\n",
      "        print \"indexing: \" + str(eis)\n",
      "        report = models.Report.objects(eis_number=str(eis)).first()\n",
      "        if report:\n",
      "            doc = {\n",
      "                \"agency_abbrev\": report.agency[0],\n",
      "                \"amended_notice\": report.amended_notice,\n",
      "                \"amended_notice_date\": report.amended_notice_date,\n",
      "                \"comment_due_review_date\": report.comment_due_review_date,\n",
      "                \"comment_letter_date\": report.comment_letter_date,\n",
      "                \"contact_name\": report.contact_name,\n",
      "                \"contact_phone\": report.contact_phone,\n",
      "                \"date_uploaded\": report.date_uploaded,\n",
      "                \"date\": report.federal_register_date,\n",
      "                \"document_type\": report.document_type,\n",
      "                \"eis_number\": report.eis_number,\n",
      "                \"federal_register_date\": report.federal_register_date,\n",
      "                \"rating\": report.rating,\n",
      "                \"report_link\": report.report_link,\n",
      "                \"state_abbrev\": report.state[0],\n",
      "                \"supplemental_info\": report.supplemental_info.encode('utf-8'),\n",
      "                \"title\": report.title,\n",
      "                \"website\": report.website,\n",
      "            }\n",
      "            res = es.index(index=INDEX, doc_type=PARENT, id=eis, body=doc)\n",
      "            print res['created']\n",
      "        else: \n",
      "            print \"Report not found: \" + str(eis)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index_missing_reports()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "indexing: 20140362\n",
        "{u'_type': u'report', u'_id': u'20140362', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140367\n",
        "{u'_type': u'report', u'_id': u'20140367', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140364\n",
        "{u'_type': u'report', u'_id': u'20140364', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140368\n",
        "{u'_type': u'report', u'_id': u'20140368', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140372\n",
        "{u'_type': u'report', u'_id': u'20140372', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140359\n",
        "{u'_type': u'report', u'_id': u'20140359', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140363\n",
        "{u'_type': u'report', u'_id': u'20140363', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140358\n",
        "{u'_type': u'report', u'_id': u'20140358', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140369\n",
        "{u'_type': u'report', u'_id': u'20140369', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140366\n",
        "{u'_type': u'report', u'_id': u'20140366', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140371\n",
        "{u'_type': u'report', u'_id': u'20140371', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140361\n",
        "{u'_type': u'report', u'_id': u'20140361', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140360\n",
        "{u'_type': u'report', u'_id': u'20140360', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indexing: 20140370\n",
        "{u'_type': u'report', u'_id': u'20140370', u'created': False, u'_version': 3, u'_index': u'impactstatement'}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def index_missing_report_files():\n",
      "    data = pd.read_csv('reports_to_be_added_to_mongo.csv')\n",
      "    data = data.fillna('')\n",
      "    missing_reports = [row[1]['eis_number'] for row in data.T.iteritems()]\n",
      "    for eis in missing_reports:\n",
      "        report = models.Report.objects(eis_number=str(eis)).first()\n",
      "        if report:\n",
      "            for f in report.report_files:\n",
      "                file_url = f.file_url_s3.replace(\"https://s3.amazonaws.com/epaeis/\",\"\").replace(\".pdf\",\".txt\")\n",
      "                key = bucket.get_key(file_url)\n",
      "                temp_file = key.get_contents_to_filename('temp.txt')\n",
      "                file64 = open('temp.txt', \"rb\").read().encode(\"base64\")\n",
      "                filejson = open('temp.json', 'w')\n",
      "                data = { \n",
      "                    'file': file64, \n",
      "                    'title': f.title, \n",
      "                    'file_url':f.file_url_s3,\n",
      "                    'eis_number': eis\n",
      "                }\n",
      "                print \"Indexing: \" + file_url\n",
      "                json.dump(data, filejson)\n",
      "                filejson.close()\n",
      "                cmd = 'curl -u {}:{} -X POST \"{}/{}/{}?parent={}\" -d @'.format(USER, PASS, HOST, INDEX, \"eis_file\", eis)\n",
      "                cmd += 'temp.json'\n",
      "                os.system(cmd)\n",
      "                os.remove('temp.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index_missing_report_files()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}