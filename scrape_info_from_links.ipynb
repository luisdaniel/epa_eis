{
 "metadata": {
  "name": "",
  "signature": "sha256:78b18065f59d27434e9af80b19d3b65b0f9f21d3ddc80a66e1828c1a0e2204ee"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup as bs\n",
      "import re\n",
      "import urllib2\n",
      "import csv\n",
      "import json\n",
      "import datetime\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns = ['date', 'agency', 'state', 'document_type', 'title', 'report_link', \n",
      "           'eis_number', 'federal_register_date', 'contact_name', 'comment_due_review_date', 'contact_phone', \n",
      "           'amended_notice_date', 'amended_notice', 'supplemental_info', 'website', 'comment_letter_date', \n",
      "           'rating', 'num_comment_letter', 'comment_letter_links', 'num_files', 'list_of_links']\n",
      "df = pd.DataFrame(columns=columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this is a small script to check whether links were already scraped or not. Checks against first run of script.\n",
      "file_name = 'reports_1_2.csv'\n",
      "reports_we_have = pd.read_csv(file_name)\n",
      "urls = set(pd.Series(reports_we_have['report_link']))\n",
      "def scraped(url):\n",
      "    return url in urls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#test\n",
      "url = 'http://yosemite.epa.gov/oeca/webeis.nsf/EIS01/9F1351654AE8BA1485257C82002113CF'\n",
      "#url_2 = 'http://yosemite.epa.gov/oeca/webeis.nsf/EIS01/C3B3CE406F34789F85257D8700216C98'\n",
      "page = urllib2.urlopen(url).read()\n",
      "soup = bs(page)\n",
      "table = soup.findAll('td')\n",
      "check_row = table[30]\n",
      "comment_letter_links = []\n",
      "document_links = []\n",
      "exclude_links = ['http://www.epa.gov/epahome/pdf.html',\n",
      "                 'http://www.epa.gov/compliance/contact/nepa.html#commentform']\n",
      "if check_row.find(text=re.compile(r'\\bComment\\sLetter\\(s\\)')):\n",
      "    base_file_url = 'http://yosemite.epa.gov/oeca/webeis.nsf/'\n",
      "    links = table[30].findAll('a')\n",
      "    for link in links:\n",
      "        if link['href'] not in exclude_links:\n",
      "            comment_letter_links.append(base_file_url + urllib2.quote(link['href'].encode('utf8').replace('?OpenElement', '').replace('../','')))\n",
      "    links = table[31].findAll('a')\n",
      "    for link in links:\n",
      "        document_links.append(base_file_url + urllib2.quote(link['href'].encode('utf8').replace('?OpenElement', '').replace('../','')))\n",
      "else: \n",
      "    links = table[30].findAll('a')\n",
      "    for link in links:\n",
      "        document_links.append(base_file_url + urllib2.quote(link['href'].encode('utf8').replace('?OpenElement', '').replace('../','')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "table[31]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "<td colspan=\"4\" width=\"100%\"><strong><u>EIS Document(s):</u></strong><br>\n",
        "<span class=\"viewdata\"><a href=\"../(EISDocs)/20140172/$file/Double-Crested Cormorant Management Plan to Reduce Predation of Juvenile Salmonids in the Columbia River Estuary_Draft Environmental Impact Statement.pdf?OpenElement\">EIS Document: Double-Crested Cormorant Management Plan to Reduce Predation of Juvenile Salmonids in the Columbia River Estuary_Draft Environmental Impact Statement.pdf</a> <br/></span></br></td>"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#1 - date\n",
      "#2 - agency\n",
      "#3 - state\n",
      "#4 - document_type\n",
      "#5 - title\n",
      "#6 - report_link\n",
      "#\"\"\"'eis_number', 'federal_register_date', 'contact_name', 'comment_due_review_date', 'contact_phone', \n",
      "#           'amended_notice_date', 'amended_notice', 'supplemental_info', 'website', 'comment_letter_date', 'comment_letter',\n",
      "#           'rating', 'num_files', 'list_of_links'\"\"\"\n",
      "\n",
      "\n",
      "#EPA server won't let you get all the URL's. Had to split it in two. \n",
      "index = 0\n",
      "with open('eis_links.csv', mode='r') as infile:\n",
      "    reader = csv.reader(infile)\n",
      "    for row in reader:\n",
      "        if not scraped(row[6]) and row[6] != 'report_link': #only used when split in parts\n",
      "            #if row[6] != 'report_link':\n",
      "            print \"Getting # \" + str(index) + \": \" + row[6]\n",
      "            page = urllib2.urlopen(row[6]).read()\n",
      "            soup = bs(page)\n",
      "            table = soup.findAll('td')\n",
      "            title = table[1].text\n",
      "            eis_number = table[3].text\n",
      "            state = table[5].text\n",
      "            document_type = table[7].text\n",
      "            agency = table[9].text\n",
      "            federal_register_date = table[11].text\n",
      "            contact_name = table[13].text\n",
      "            comment_due_review_date = table[15].text\n",
      "            contact_phone = table[17].text\n",
      "            amended_notice_date = table[19].text\n",
      "            amended_notice = table[21].text\n",
      "            supplemental_info = table[23].text\n",
      "            website = table[25].text.rstrip()\n",
      "            comment_letter_date = table[27].text.rstrip()\n",
      "            rating = table[29].text\n",
      "            #sometimes you have comment letters which changes the order of the table rows\n",
      "            check_row = table[30]\n",
      "            comment_letter_links = []\n",
      "            document_links = []\n",
      "            exclude_links = ['http://www.epa.gov/epahome/pdf.html',\n",
      "                             'http://www.epa.gov/compliance/contact/nepa.html#commentform']\n",
      "            if check_row.find(text=re.compile(r'\\bComment\\sLetter\\(s\\)')):\n",
      "                base_file_url = 'http://yosemite.epa.gov/oeca/webeis.nsf/'\n",
      "                links = table[30].findAll('a')\n",
      "                for link in links:\n",
      "                    if link['href'] not in exclude_links:\n",
      "                        comment_letter_links.append(base_file_url + urllib2.quote(link['href'].encode('utf8').replace('?OpenElement', '').replace('../','')))\n",
      "                links = table[31].findAll('a')\n",
      "                for link in links:\n",
      "                    document_links.append(base_file_url + urllib2.quote(link['href'].encode('utf8').replace('?OpenElement', '').replace('../','')))\n",
      "            else: \n",
      "                links = table[30].findAll('a')\n",
      "                for link in links:\n",
      "                    document_links.append(base_file_url + urllib2.quote(link['href'].encode('utf8').replace('?OpenElement', '').replace('../','')))\n",
      "            num_comment_letter = len(comment_letter_links)\n",
      "            comment_letter_links = ' '.join(comment_letter_links)\n",
      "            num_documents = len(document_links)\n",
      "            document_links = ' '.join(document_links)\n",
      "            new_row = [row[1], row[2], row[3], row[4], row[5], row[6],\n",
      "                       eis_number, federal_register_date, contact_name, comment_due_review_date, contact_phone, \n",
      "                       amended_notice_date, amended_notice, supplemental_info, website, comment_letter_date, \n",
      "                       rating, num_comment_letter, comment_letter_links, num_documents, document_links]\n",
      "            for i in range(len(new_row)):  # For every value in our newrow\n",
      "                if hasattr(new_row[i], 'encode'):\n",
      "                    new_row[i] = new_row[i].encode('utf8')\n",
      "            df.loc[index] = new_row\n",
      "            index +=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#df.to_csv('reports_part_1.csv', encoding='utf-8')\n",
      "#df.to_csv('reports_part_2.csv', encoding='utf-8')\n",
      "df.to_csv('reports_part_3.csv', encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#join\n",
      "df1 = pd.read_csv('reports_1_2.csv')\n",
      "df2 = pd.read_csv('reports_part_3.csv')\n",
      "#remove last row so we can get the complete list of files\n",
      "df1 = df1[0:len(df1)-1]\n",
      "df2 = df2[0:len(df2)-1]\n",
      "df_join = pd.concat([df1, df2])\n",
      "df_join = df_join.drop_duplicates()\n",
      "df_join.to_csv('reports.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    }
   ],
   "metadata": {}
  }
 ]
}